{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "from models import get_model\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import preprocessor as p\n",
    "from collections import Counter\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from tensorflow.contrib import learn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from scipy import stats\n",
    "import tflearn\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    filename = get_filename(dataset)\n",
    "    print(\"Loading data from file: \" + filename)\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    x_text = []\n",
    "    labels = [] \n",
    "    for i in range(len(data)):\n",
    "        if(HASH_REMOVE):\n",
    "            x_text.append(p.tokenize((data[i]['text']).encode('utf-8')))\n",
    "        else:\n",
    "            x_text.append(data[i]['text'])\n",
    "        labels.append(data[i]['label'])\n",
    "    if dataset == \"wiki\":\n",
    "        reduced_number_of_samples = int(len(x_text) * 0.5)\n",
    "        x_text, labels = shuffle(x_text, labels, \n",
    "            random_state=42, \n",
    "            n_samples=reduced_number_of_samples)\n",
    "        print \"WARNING: Wiki data set reduced from %d to %d number of samples!\" % (len(data), reduced_number_of_samples)\n",
    "        \n",
    "\n",
    "    return x_text,labels\n",
    "\n",
    "def get_filename(dataset):\n",
    "    global NUM_CLASSES, HASH_REMOVE\n",
    "    if(dataset==\"twitter\"):\n",
    "        NUM_CLASSES = 3\n",
    "        HASH_REMOVE = True\n",
    "        filename = \"data/twitter_data.pkl\"\n",
    "    elif(dataset==\"formspring\"):\n",
    "        NUM_CLASSES = 2\n",
    "        filename = \"data/formspring_data.pkl\"\n",
    "    elif(dataset==\"wiki\"):\n",
    "        NUM_CLASSES = 2\n",
    "        filename = \"data/wiki_data.pkl\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_weights(filename, sep):\n",
    "    embed_dict = {}\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(sep)\n",
    "        embed_dict[row[0]] = row[1:]\n",
    "    print('Loaded from file: ' + str(filename))\n",
    "    file.close()\n",
    "    return embed_dict\n",
    "\n",
    "def map_embedding_weights(embed, vocab, embed_size):\n",
    "    vocab_size = len(vocab)\n",
    "    embeddingWeights = np.zeros((vocab_size , embed_size))\n",
    "    n = 0\n",
    "    words_missed = []\n",
    "    for k, v in vocab.iteritems():\n",
    "        try:\n",
    "            embeddingWeights[v] = embed[k]\n",
    "        except:\n",
    "            n += 1\n",
    "            words_missed.append(k)\n",
    "            pass\n",
    "    print(\"%d embedding missed\"%n, \" of \" , vocab_size)\n",
    "    return embeddingWeights\n",
    "\n",
    "def get_embeddings_dict(vector_type, emb_dim):\n",
    "    if vector_type == 'sswe':\n",
    "        emb_dim==50\n",
    "        sep = '\\t'\n",
    "        vector_file = 'word_vectors/sswe-u.txt'\n",
    "    elif vector_type ==\"glove\":\n",
    "        sep = ' '\n",
    "        if data == \"wiki\":\n",
    "            vector_file = 'word_vectors/glove.6B.' + str(emb_dim) + 'd.txt'\n",
    "        else:\n",
    "            vector_file = 'word_vectors/glove.twitter.27B.' + str(emb_dim) + 'd.txt'\n",
    "    else:\n",
    "        print \"ERROR: Please specify a correst model or SSWE cannot be loaded with embed size of: \" + str(emb_dim) \n",
    "        return None\n",
    "    \n",
    "    embed = get_embedding_weights(vector_file, sep)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, testX, testY, dump_results, dump_results_file_name):\n",
    "    temp = model.predict(testX)\n",
    "    y_pred  = np.argmax(temp, 1)\n",
    "    y_true = np.argmax(testY, 1)\n",
    "    if dump_results:\n",
    "        pd.DataFrame(data={\n",
    "            \"y_true\": y_true,\n",
    "            \"y_pred\": y_pred\n",
    "        }).to_csv(dump_results_file_name)\n",
    "        print(\"Writter results to \\\"\" + dump_results_file_name + \"\\\"\")\n",
    "    if(data == \"twitter\"):\n",
    "        precision = metrics.precision_score(y_true, y_pred, average=None, labels=[0, 2, 1])\n",
    "        recall = metrics.recall_score(y_true, y_pred, average=None, labels=[0, 2, 1])\n",
    "        f1_score = metrics.f1_score(y_true, y_pred, average=None, labels=[0, 2, 1])\n",
    "    else:\n",
    "        precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "        recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "        f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "    print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "    print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "    print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\":: Classification Report\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_learned_embedding(data, model_type, vector_type, embed_size, embed, vocab_processor):\n",
    "    vocab = vocab_processor.vocabulary_._mapping\n",
    "    vocab_size = len(vocab)\n",
    "    embedDict = {}\n",
    "    n = 0\n",
    "    words_missed = []\n",
    "    for k, v in vocab.iteritems():\n",
    "        try:\n",
    "            embeddingDict[v] = embed[k]\n",
    "        except:\n",
    "            n += 1\n",
    "            words_missed.append(k)\n",
    "            pass\n",
    "    print(\"%d embedding missed\"%n, \" of \" , vocab_size)\n",
    "    \n",
    "    filename = output_folder_name + data + \"_\" + model_type + \"_\" + vector_type + \"_\" + embed_size + \".pkl\"\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(embedDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(data, x_text, labels, oversampling_rate):\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split( x_text, labels, random_state=42, test_size=0.10)\n",
    "\n",
    "    X_train, Y_train = do_oversampling(data, oversampling_rate, Y_train, X_train)\n",
    "    \n",
    "    post_length = np.array([len(x.split(\" \")) for x in x_text])\n",
    "    if(data != \"twitter\"):\n",
    "        max_document_length = int(np.percentile(post_length, 95))\n",
    "    else:\n",
    "        max_document_length = max(post_length)\n",
    "    print(\"Document length : \" + str(max_document_length))\n",
    "    \n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, MAX_FEATURES)\n",
    "    vocab_processor = vocab_processor.fit(x_text)\n",
    "\n",
    "    trainX = np.array(list(vocab_processor.transform(X_train)))\n",
    "    testX = np.array(list(vocab_processor.transform(X_test)))\n",
    "    \n",
    "    trainY = np.asarray(Y_train)\n",
    "    testY = np.asarray(Y_test)\n",
    "        \n",
    "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "\n",
    "    trainY = to_categorical(trainY, nb_classes=NUM_CLASSES)\n",
    "    testY = to_categorical(testY, nb_classes=NUM_CLASSES)\n",
    "    \n",
    "    data_dict = {\n",
    "        \"data\": data,\n",
    "        \"trainX\" : trainX,\n",
    "        \"trainY\" : trainY,\n",
    "        \"testX\" : testX,\n",
    "        \"testY\" : testY,\n",
    "        \"vocab_processor\" : vocab_processor\n",
    "    }\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_data(data_dict):\n",
    "    return data_dict[\"data\"], data_dict[\"trainX\"], data_dict[\"trainY\"], data_dict[\"testX\"], data_dict[\"testY\"], data_dict[\"vocab_processor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_weights(model, weights=None):\n",
    "    \"\"\"Randomly permute the weights in `model`, or the given `weights`.\n",
    "    This is a fast approximation of re-initializing the weights of a model.\n",
    "    Assumes weights are distributed independently of the dimensions of the weight tensors\n",
    "      (i.e., the weights have the same distribution along each dimension).\n",
    "    :param Model model: Modify the weights of the given model.\n",
    "    :param list(ndarray) weights: The model's weights will be replaced by a random permutation of these weights.\n",
    "      If `None`, permute the model's current weights.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    # Faster, but less random: only permutes along the first dimension\n",
    "    # weights = [np.random.permutation(w) for w in weights]\n",
    "    model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_dict, model_type, vector_type, embed_size, dump_results=False, dump_results_file_name=\"\"):\n",
    "\n",
    "    data, trainX, trainY, testX, testY, vocab_processor = return_data(data_dict)\n",
    "    \n",
    "    vocab_size = len(vocab_processor.vocabulary_)\n",
    "    print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "    vocab = vocab_processor.vocabulary_._mapping\n",
    "    \n",
    "    print(\"Running Model: \" + model_type + \" with word vector initiliazed with \" + vector_type + \" word vectors.\")\n",
    "    model = get_model(model_type, trainX.shape[1], vocab_size, embed_size, NUM_CLASSES, LEARN_RATE)\n",
    "\n",
    "    if(model_type != \"cnn\"):\n",
    "        #This is a very crude bugfix, since this method doesnt exist in the DNN class of tflearn\n",
    "        initial_weights = model.get_weights()\n",
    "        shuffle_weights(model, initial_weights)\n",
    "    \n",
    "    if(model_type == 'cnn'):\n",
    "        if(vector_type!=\"random\"):\n",
    "            print(\"Word vectors used: \" + vector_type)\n",
    "            embeddingWeights = tflearn.get_layer_variables_by_name('EmbeddingLayer')[0]\n",
    "            model.set_weights(embeddingWeights, map_embedding_weights(get_embeddings_dict(vector_type, embed_size), vocab, embed_size))\n",
    "            model.fit(trainX, trainY, n_epoch = EPOCHS, shuffle=True, show_metric=True, batch_size=BATCH_SIZE)\n",
    "        else:\n",
    "            model.fit(trainX, trainY, n_epoch = EPOCHS, shuffle=True, show_metric=True, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        if(vector_type!=\"random\"):\n",
    "            print(\"Word vectors used: \" + vector_type)\n",
    "            model.layers[0].set_weights([map_embedding_weights(get_embeddings_dict(vector_type, embed_size), vocab, embed_size)])\n",
    "            model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
    "                  verbose=1)\n",
    "        else:\n",
    "            model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
    "                  verbose=1)\n",
    "    \n",
    "    return  evaluate_model(model, testX, testY, dump_results, dump_results_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(precision_scores, recall_scores, f1_scores):\n",
    "    for i in range(NUM_CLASSES):\n",
    "        print(\"\\nPrecision Class %d (avg): %0.3f (+/- %0.3f)\" % (i, precision_scores[:, i].mean(), precision_scores[:, i].std() * 2))\n",
    "        print( \"\\nRecall Class %d (avg): %0.3f (+/- %0.3f)\" % (i, recall_scores[:, i].mean(), recall_scores[:, i].std() * 2))\n",
    "        print( \"\\nF1 score Class %d (avg): %0.3f (+/- %0.3f)\" % (i, f1_scores[:, i].mean(), f1_scores[:, i].std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_oversampling(data, oversampling_rate, labels, x_text): \n",
    "    if(data==\"twitter\"):\n",
    "        NUM_CLASSES = 3\n",
    "        \n",
    "        racism = [i for i in range(len(labels)) if labels[i]==2]\n",
    "        sexism = [i for i in range(len(labels)) if labels[i]==1]\n",
    "        x_text = x_text + [x_text[x] for x in racism]*(oversampling_rate-1)+ [x_text[x] for x in sexism]*(oversampling_rate-1)\n",
    "        labels = labels + [2 for i in range(len(racism))]*(oversampling_rate-1) + [1 for i in range(len(sexism))]*(oversampling_rate-1)\n",
    "    else:  \n",
    "        NUM_CLASSES = 2\n",
    "        bully = [i for i in range(len(labels)) if labels[i]==1]\n",
    "        x_text = x_text + [x_text[x] for x in bully]*(oversampling_rate-1)\n",
    "        labels = list(labels) + [1 for i in range(len(bully))]*(oversampling_rate-1)\n",
    "\n",
    "    print(\"Counter after oversampling\")\n",
    "    from collections import Counter\n",
    "    print(Counter(labels))\n",
    "    \n",
    "    #this is never used again so im uncommenting it\n",
    "    #filter_data = []\n",
    "    #for text in x_text:\n",
    "    #    filter_data.append(\"\".join(l for l in text if l not in string.punctuation))\n",
    "        \n",
    "    return x_text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ 'cnn', 'lstm', 'blstm', 'blstm_attention']\n",
    "word_vectors = [\"random\", \"glove\" ,\"sswe\"]\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "MAX_FEATURES = 2\n",
    "NUM_CLASSES = None\n",
    "DROPOUT = 0.25\n",
    "LEARN_RATE = 0.01\n",
    "HASH_REMOVE = None\n",
    "output_folder_name = \"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data, oversampling_rate, model_type, vector_type, embed_size, dump_results=False, dump_results_file_name=\"\"):\n",
    "    x_text, labels = load_data(data) \n",
    "    if data == \"twitter\":\n",
    "        #map labels\n",
    "        dict1 = {'racism':2,'sexism':1,'none':0}\n",
    "        labels = [dict1[b] for b in labels]\n",
    "    data_dict = get_train_test(data,  x_text, labels, oversampling_rate)\n",
    "    precision, recall, f1_score = train(data_dict, model_type, vector_type, embed_size, dump_results, dump_results_file_name)\n",
    "    return precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: data/wiki_data.pkl\n",
      "WARNING: Wiki data set reduced from 115864 to 57932 number of samples!\n",
      "Counter after oversampling\n",
      "Counter({0: 46014, 1: 18372})\n",
      "Document length : 238\n",
      "Vocabulary Size: 37183\n",
      "Running Model: blstm_attention with word vector initiliazed with glove word vectors.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 238, 100)          3718300   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 238, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 238, 200)          160800    \n",
      "_________________________________________________________________\n",
      "att_layer_3 (AttLayer)       (None, 200)               200       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 3,879,702\n",
      "Trainable params: 3,879,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Word vectors used: glove\n",
      "Loaded from file: word_vectors/glove.6B.100d.txt\n",
      "('5577 embedding missed', ' of ', 37183)\n",
      "Epoch 1/10\n",
      "64386/64386 [==============================] - 1278s - loss: 0.3127 - acc: 0.8676  \n",
      "Epoch 2/10\n",
      "64386/64386 [==============================] - 1343s - loss: 0.1794 - acc: 0.9298  \n",
      "Epoch 3/10\n",
      "64386/64386 [==============================] - 1152s - loss: 0.1296 - acc: 0.9507  \n",
      "Epoch 4/10\n",
      "64386/64386 [==============================] - 1148s - loss: 0.0920 - acc: 0.9670  \n",
      "Epoch 5/10\n",
      "64386/64386 [==============================] - 1152s - loss: 0.0701 - acc: 0.9751  \n",
      "Epoch 6/10\n",
      "64386/64386 [==============================] - 1164s - loss: 0.0576 - acc: 0.9806  \n",
      "Epoch 7/10\n",
      "64386/64386 [==============================] - 1201s - loss: 0.0475 - acc: 0.9842  \n",
      "Epoch 8/10\n",
      "64386/64386 [==============================] - 1451s - loss: 0.0397 - acc: 0.9867  \n",
      "Epoch 9/10\n",
      "64386/64386 [==============================] - 1467s - loss: 0.0334 - acc: 0.9894  \n",
      "Epoch 10/10\n",
      "64386/64386 [==============================] - 1433s - loss: 0.0327 - acc: 0.9899  \n",
      "Precision: [0.96233741 0.72317263]\n",
      "\n",
      "Recall: [0.96533593 0.70561457]\n",
      "\n",
      "f1_score: [0.96383434 0.71428571]\n",
      "\n",
      "[[4957  178]\n",
      " [ 194  465]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.97      0.96      5135\n",
      "          1       0.72      0.71      0.71       659\n",
      "\n",
      "avg / total       0.94      0.94      0.94      5794\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.96233741, 0.72317263]),\n",
       " array([0.96533593, 0.70561457]),\n",
       " array([0.96383434, 0.71428571]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"wiki\"\n",
    "model_type = \"blstm_attention\"\n",
    "vector_type = \"glove\"\n",
    "embed_size = 100\n",
    "run_model(data, 3, model_type, vector_type, embed_size,\n",
    "         True,\n",
    "          \"dumps/%s_%d_%s_%s.csv\" % (data, oversampling_rate, vector_type, model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: data/wiki_data.pkl\n",
      "WARNING: Wiki data set reduced from 115864 to 57932 number of samples!\n",
      "Counter after oversampling\n",
      "Counter({0: 46014, 1: 18372})\n",
      "Document length : 238\n",
      "Vocabulary Size: 37183\n",
      "Running Model: blstm with word vector initiliazed with glove word vectors.\n",
      "Word vectors used: glove\n",
      "Loaded from file: word_vectors/glove.6B.100d.txt\n",
      "('5577 embedding missed', ' of ', 37183)\n",
      "Epoch 1/10\n",
      "64386/64386 [==============================] - 1394s - loss: 0.2880 - acc: 0.8800  \n",
      "Epoch 2/10\n",
      "64386/64386 [==============================] - 1873s - loss: 0.1704 - acc: 0.9311  \n",
      "Epoch 3/10\n",
      "64386/64386 [==============================] - 2071s - loss: 0.1213 - acc: 0.9529  \n",
      "Epoch 4/10\n",
      "64386/64386 [==============================] - 2373s - loss: 0.0874 - acc: 0.9681  \n",
      "Epoch 5/10\n",
      "64386/64386 [==============================] - 2598s - loss: 0.0692 - acc: 0.9760  \n",
      "Epoch 6/10\n",
      "64386/64386 [==============================] - 2734s - loss: 0.0558 - acc: 0.9815  \n",
      "Epoch 7/10\n",
      "64386/64386 [==============================] - 2614s - loss: 0.0470 - acc: 0.9845  \n",
      "Epoch 8/10\n",
      "64386/64386 [==============================] - 2573s - loss: 0.0383 - acc: 0.9872  \n",
      "Epoch 9/10\n",
      "64386/64386 [==============================] - 2460s - loss: 0.0314 - acc: 0.9894  \n",
      "Epoch 10/10\n",
      "64386/64386 [==============================] - 2533s - loss: 0.0277 - acc: 0.9904  \n",
      "Precision: [0.9627907  0.73659306]\n",
      "\n",
      "Recall: [0.96747809 0.70864947]\n",
      "\n",
      "f1_score: [0.9651287  0.72235112]\n",
      "\n",
      "[[4968  167]\n",
      " [ 192  467]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.97      0.97      5135\n",
      "          1       0.74      0.71      0.72       659\n",
      "\n",
      "avg / total       0.94      0.94      0.94      5794\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.9627907 , 0.73659306]),\n",
       " array([0.96747809, 0.70864947]),\n",
       " array([0.9651287 , 0.72235112]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"wiki\"\n",
    "model_type = \"blstm\"\n",
    "vector_type = \"glove\"\n",
    "embed_size = 100\n",
    "run_model(data, 3, model_type, vector_type, embed_size,\n",
    "          True, \n",
    "          \"dumps/%s_%d_%s_%s.csv\" % (data, oversampling_rate, vector_type, model_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing table 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5039  | total loss: \u001b[1m\u001b[32m0.13702\u001b[0m\u001b[0m | time: 116.043s\n",
      "| Adam | epoch: 010 | loss: 0.13702 - acc: 0.9494 -- iter: 64384/64386\n",
      "Training Step: 5040  | total loss: \u001b[1m\u001b[32m0.15092\u001b[0m\u001b[0m | time: 116.258s\n",
      "| Adam | epoch: 010 | loss: 0.15092 - acc: 0.9443 -- iter: 64386/64386\n",
      "--\n",
      "Writter results to \"dumps/wiki_3_glove_cnn.csv\"\n",
      "Precision: [0.95613369 0.81060606]\n",
      "\n",
      "Recall: [0.9805258  0.64946889]\n",
      "\n",
      "f1_score: [0.96817614 0.72114575]\n",
      "\n",
      "[[5035  100]\n",
      " [ 231  428]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.98      0.97      5135\n",
      "          1       0.81      0.65      0.72       659\n",
      "\n",
      "avg / total       0.94      0.94      0.94      5794\n",
      "\n",
      "Results done for wiki 3 with vector type glove and model cnn\n",
      "Loading data from file: data/wiki_data.pkl\n",
      "WARNING: Wiki data set reduced from 115864 to 57932 number of samples!\n",
      "Counter after oversampling\n",
      "Counter({0: 46014, 1: 18372})\n",
      "Document length : 238\n",
      "Vocabulary Size: 37183\n",
      "Running Model: blstm_attention with word vector initiliazed with glove word vectors.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 238, 100)          3718300   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 238, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 238, 200)          160800    \n",
      "_________________________________________________________________\n",
      "att_layer_5 (AttLayer)       (None, 200)               200       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 3,879,702\n",
      "Trainable params: 3,879,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Word vectors used: glove\n",
      "Loaded from file: word_vectors/glove.6B.100d.txt\n",
      "('5577 embedding missed', ' of ', 37183)\n",
      "Epoch 1/10\n",
      "64386/64386 [==============================] - 1421s - loss: 0.3262 - acc: 0.8605  \n",
      "Epoch 2/10\n",
      "64386/64386 [==============================] - 1638s - loss: 0.1908 - acc: 0.9235  \n",
      "Epoch 3/10\n",
      "64386/64386 [==============================] - 1254s - loss: 0.1304 - acc: 0.9487  \n",
      "Epoch 4/10\n",
      "64386/64386 [==============================] - 1245s - loss: 0.0935 - acc: 0.9655  \n",
      "Epoch 5/10\n",
      "64386/64386 [==============================] - 1253s - loss: 0.0746 - acc: 0.9735  \n",
      "Epoch 6/10\n",
      "64386/64386 [==============================] - 1262s - loss: 0.0592 - acc: 0.9796  \n",
      "Epoch 7/10\n",
      "50304/64386 [======================>.......] - ETA: 279s - loss: 0.0475 - acc: 0.9839"
     ]
    }
   ],
   "source": [
    "data_sets = [\"wiki\"]\n",
    "model_types = [\"cnn\", \"blstm_attention\"]\n",
    "vector_types = [\"random\", \"glove\"]\n",
    "#fixed embed size for faster computation\n",
    "embed_size = 100\n",
    "results_table_4 = []\n",
    "for data in data_sets:\n",
    "    for taken_oversampling_rate in [1, 3]:\n",
    "        for vector_type in vector_types:\n",
    "            for model_type in model_types:\n",
    "                precision, recall, f1_score = run_model(data, \n",
    "                                                        taken_oversampling_rate, \n",
    "                                                        model_type, \n",
    "                                                        vector_type, \n",
    "                                                        embed_size, \n",
    "                                                        True, \n",
    "                                                        \"dumps/%s_%d_%s_%s.csv\" % (data, \n",
    "                                                                                   taken_oversampling_rate, \n",
    "                                                                                   vector_type, \n",
    "                                                                                   model_type))\n",
    "                print \"Results done for %s %d with vector type %s and model %s\" % (data, \n",
    "                                                                                   taken_oversampling_rate, \n",
    "                                                                                   vector_type, \n",
    "                                                                                   model_type)\n",
    "                results_table_4.append({\n",
    "                    \"dataset\": data,\n",
    "                    \"oversampling_rate\": taken_oversampling_rate,\n",
    "                    \"vector_type\": vector_type,\n",
    "                    \"model_type\": model_type,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"f1_score\": f1_score\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d3f7dc091f1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"blstm\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvector_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"glove\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'run_model' is not defined"
     ]
    }
   ],
   "source": [
    "data = \"wiki\"\n",
    "model_type = \"blstm\"\n",
    "vector_type = \"glove\"\n",
    "run_model(data, 3, model_type, vector_type, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing table 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: data/twitter_data.pkl\n",
      "Counter after oversampling\n",
      "Counter({0: 9946, 1: 8415, 2: 5190})\n",
      "Document length : 38\n",
      "Vocabulary Size: 5711\n",
      "Running Model: blstm with word vector initiliazed with glove word vectors.\n",
      "Word vectors used: glove\n",
      "Loaded from file: word_vectors/glove.twitter.27B.100d.txt\n",
      "('217 embedding missed', ' of ', 5711)\n",
      "Epoch 1/10\n",
      "23551/23551 [==============================] - 76s - loss: 0.6756 - acc: 0.6912    \n",
      "Epoch 2/10\n",
      "23551/23551 [==============================] - 78s - loss: 0.4848 - acc: 0.8002    \n",
      "Epoch 3/10\n",
      "23551/23551 [==============================] - 67s - loss: 0.4053 - acc: 0.8387    \n",
      "Epoch 4/10\n",
      "23551/23551 [==============================] - 81s - loss: 0.3454 - acc: 0.8625    \n",
      "Epoch 5/10\n",
      "23551/23551 [==============================] - 82s - loss: 0.3006 - acc: 0.8820    \n",
      "Epoch 6/10\n",
      "23551/23551 [==============================] - 82s - loss: 0.2622 - acc: 0.9015    \n",
      "Epoch 7/10\n",
      "23551/23551 [==============================] - 73s - loss: 0.2296 - acc: 0.9126    \n",
      "Epoch 8/10\n",
      "23551/23551 [==============================] - 78s - loss: 0.2051 - acc: 0.9244    \n",
      "Epoch 9/10\n",
      "23551/23551 [==============================] - 81s - loss: 0.1894 - acc: 0.9329    \n",
      "Epoch 10/10\n",
      "23551/23551 [==============================] - 76s - loss: 0.1756 - acc: 0.9369    \n",
      "Writter results to \"dumps/twitter_3_glove_blstm.csv\"\n",
      "Precision: [0.87370405 0.74647887 0.65970149]\n",
      "\n",
      "Recall: [0.85045872 0.76811594 0.70833333]\n",
      "\n",
      "f1_score: [0.86192469 0.75714286 0.68315301]\n",
      "\n",
      "[[927 112  51]\n",
      " [ 88 221   3]\n",
      " [ 46   2 159]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.85      0.86      1090\n",
      "          1       0.66      0.71      0.68       312\n",
      "          2       0.75      0.77      0.76       207\n",
      "\n",
      "avg / total       0.82      0.81      0.81      1609\n",
      "\n",
      "Results done for twitter 3 with vector type glove and model blstm\n",
      "Loading data from file: data/formspring_data.pkl\n",
      "Counter after oversampling\n",
      "Counter({0: 10787, 1: 2124})\n",
      "Document length : 62\n",
      "Vocabulary Size: 6053\n",
      "Running Model: lstm with word vector initiliazed with glove word vectors.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 62, 100)           605300    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 62, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 685,902\n",
      "Trainable params: 685,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Word vectors used: glove\n",
      "Loaded from file: word_vectors/glove.twitter.27B.100d.txt\n",
      "('335 embedding missed', ' of ', 6053)\n",
      "Epoch 1/10\n",
      "12911/12911 [==============================] - 35s - loss: 0.4636 - acc: 0.8309    \n",
      "Epoch 2/10\n",
      "12911/12911 [==============================] - 39s - loss: 0.4561 - acc: 0.8189    \n",
      "Epoch 3/10\n",
      "12911/12911 [==============================] - 37s - loss: 0.4557 - acc: 0.8353    \n",
      "Epoch 4/10\n",
      "12911/12911 [==============================] - 35s - loss: 0.4394 - acc: 0.8355    \n",
      "Epoch 5/10\n",
      "12911/12911 [==============================] - 31s - loss: 0.4332 - acc: 0.8358    \n",
      "Epoch 6/10\n",
      "12911/12911 [==============================] - 29s - loss: 0.4296 - acc: 0.8357    \n",
      "Epoch 7/10\n",
      "12911/12911 [==============================] - 30s - loss: 0.4262 - acc: 0.8360    \n",
      "Epoch 8/10\n",
      "12911/12911 [==============================] - 36s - loss: 0.4094 - acc: 0.8370    \n",
      "Epoch 9/10\n",
      "12911/12911 [==============================] - 31s - loss: 0.3783 - acc: 0.8378    \n",
      "Epoch 10/10\n",
      "12911/12911 [==============================] - 31s - loss: 0.3837 - acc: 0.8291    \n",
      "Writter results to \"dumps/formspring_3_glove_lstm.csv\"\n",
      "Precision: [0.94753328 1.        ]\n",
      "\n",
      "Recall: [1.         0.01470588]\n",
      "\n",
      "f1_score: [0.97305991 0.02898551]\n",
      "\n",
      "[[1210    0]\n",
      " [  67    1]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      1210\n",
      "          1       1.00      0.01      0.03        68\n",
      "\n",
      "avg / total       0.95      0.95      0.92      1278\n",
      "\n",
      "Results done for formspring 3 with vector type glove and model lstm\n",
      "Loading data from file: data/formspring_data.pkl\n",
      "Counter after oversampling\n",
      "Counter({0: 10787, 1: 2124})\n",
      "Document length : 62\n",
      "Vocabulary Size: 6053\n",
      "Running Model: blstm with word vector initiliazed with glove word vectors.\n",
      "Word vectors used: glove\n",
      "Loaded from file: word_vectors/glove.twitter.27B.100d.txt\n",
      "('335 embedding missed', ' of ', 6053)\n",
      "Epoch 1/10\n",
      "12911/12911 [==============================] - 61s - loss: 0.3584 - acc: 0.8588    \n",
      "Epoch 2/10\n",
      "12911/12911 [==============================] - 56s - loss: 0.2434 - acc: 0.9043    \n",
      "Epoch 3/10\n",
      "12911/12911 [==============================] - 81s - loss: 0.1974 - acc: 0.9238    \n",
      "Epoch 4/10\n",
      "12911/12911 [==============================] - 63s - loss: 0.1671 - acc: 0.9359    \n",
      "Epoch 5/10\n",
      "12911/12911 [==============================] - 73s - loss: 0.1374 - acc: 0.9465    \n",
      "Epoch 6/10\n",
      "12911/12911 [==============================] - 63s - loss: 0.1099 - acc: 0.9574    \n",
      "Epoch 7/10\n",
      "12911/12911 [==============================] - 61s - loss: 0.0933 - acc: 0.9652    \n",
      "Epoch 8/10\n",
      "12911/12911 [==============================] - 61s - loss: 0.0725 - acc: 0.9737    \n",
      "Epoch 9/10\n",
      "12911/12911 [==============================] - 62s - loss: 0.0661 - acc: 0.9761    \n",
      "Epoch 10/10\n",
      "12911/12911 [==============================] - 62s - loss: 0.0552 - acc: 0.9798    \n",
      "Writter results to \"dumps/formspring_3_glove_blstm.csv\"\n",
      "Precision: [0.97004992 0.42105263]\n",
      "\n",
      "Recall: [0.96363636 0.47058824]\n",
      "\n",
      "f1_score: [0.9668325  0.44444444]\n",
      "\n",
      "[[1166   44]\n",
      " [  36   32]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.96      0.97      1210\n",
      "          1       0.42      0.47      0.44        68\n",
      "\n",
      "avg / total       0.94      0.94      0.94      1278\n",
      "\n",
      "Results done for formspring 3 with vector type glove and model blstm\n",
      "Loading data from file: data/wiki_data.pkl\n",
      "WARNING: Wiki data set reduced from 115864 to 57932 number of samples!\n",
      "Counter after oversampling\n",
      "Counter({0: 46014, 1: 18372})\n",
      "Document length : 228\n",
      "Vocabulary Size: 36650\n",
      "Running Model: lstm with word vector initiliazed with glove word vectors.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 228, 100)          3665000   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 228, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 3,745,602\n",
      "Trainable params: 3,745,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Word vectors used: glove\n",
      "Loaded from file: word_vectors/glove.6B.100d.txt\n",
      "('5221 embedding missed', ' of ', 36650)\n",
      "Epoch 1/10\n",
      "64386/64386 [==============================] - 612s - loss: 0.6006 - acc: 0.7151   \n",
      "Epoch 2/10\n",
      "64386/64386 [==============================] - 643s - loss: 0.5890 - acc: 0.7198   \n",
      "Epoch 3/10\n",
      "64386/64386 [==============================] - 635s - loss: 0.4889 - acc: 0.7597   \n",
      "Epoch 4/10\n",
      "64386/64386 [==============================] - 638s - loss: 0.4937 - acc: 0.7195   \n",
      "Epoch 5/10\n",
      "64386/64386 [==============================] - 605s - loss: 0.4288 - acc: 0.7642   \n",
      "Epoch 6/10\n",
      "64386/64386 [==============================] - 609s - loss: 0.4115 - acc: 0.7890   \n",
      "Epoch 7/10\n",
      "64386/64386 [==============================] - 663s - loss: 0.5270 - acc: 0.7559   \n",
      "Epoch 8/10\n",
      "64386/64386 [==============================] - 659s - loss: 0.4023 - acc: 0.8469   \n",
      "Epoch 9/10\n",
      "64386/64386 [==============================] - 711s - loss: 0.3140 - acc: 0.8742   \n",
      "Epoch 10/10\n",
      "64386/64386 [==============================] - 626s - loss: 0.2284 - acc: 0.9163   \n",
      "Writter results to \"dumps/wiki_3_glove_lstm.csv\"\n",
      "Precision: [0.96714258 0.72099853]\n",
      "\n",
      "Recall: [0.96299903 0.74506829]\n",
      "\n",
      "f1_score: [0.96506635 0.73283582]\n",
      "\n",
      "[[4945  190]\n",
      " [ 168  491]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.96      0.97      5135\n",
      "          1       0.72      0.75      0.73       659\n",
      "\n",
      "avg / total       0.94      0.94      0.94      5794\n",
      "\n",
      "Results done for wiki 3 with vector type glove and model lstm\n",
      "Loading data from file: data/wiki_data.pkl\n",
      "WARNING: Wiki data set reduced from 115864 to 57932 number of samples!\n",
      "Counter after oversampling\n",
      "Counter({0: 46014, 1: 18372})\n",
      "Document length : 228\n",
      "Vocabulary Size: 36650\n",
      "Running Model: blstm with word vector initiliazed with glove word vectors.\n",
      "Word vectors used: glove\n",
      "Loaded from file: word_vectors/glove.6B.100d.txt\n",
      "('5221 embedding missed', ' of ', 36650)\n",
      "Epoch 1/10\n",
      "64386/64386 [==============================] - 1340s - loss: 0.2889 - acc: 0.8809  \n",
      "Epoch 2/10\n",
      "64386/64386 [==============================] - 1450s - loss: 0.1717 - acc: 0.9329  \n",
      "Epoch 3/10\n",
      "33280/64386 [==============>...............] - ETA: 789s - loss: 0.1301 - acc: 0.9500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-894974df381e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                         \u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                                         \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                                         \"dumps/%s_%d_%s_%s.csv\" % (data, oversampling_rate, vector_type, model_type))\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Results done for %s %d with vector type %s and model %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moversampling_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 results_table_6.append({\n",
      "\u001b[0;32m<ipython-input-13-12a9ed9b97df>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(data, oversampling_rate, model_type, vector_type, embed_size, dump_results, dump_results_file_name)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdict1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mx_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moversampling_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdump_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdump_results_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9de9f2eed49a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_dict, model_type, vector_type, embed_size, dump_results, dump_results_file_name)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmap_embedding_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_embeddings_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n\u001b[0;32m---> 30\u001b[0;31m                   verbose=1)\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/tensor/blas.pyc\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \u001b[0;31m# The error raised by numpy has no shape information, we mean to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_sets = [\"twitter\", \"formspring\", \"wiki\"]\n",
    "model_types = [ 'lstm', 'blstm']\n",
    "vector_types = [\"glove\"]\n",
    "#fixed embed size for faster computation\n",
    "embed_size = 100\n",
    "results_table_6 = []\n",
    "skip = 1\n",
    "for data in data_sets:\n",
    "    for oversampling_rate in [3]:\n",
    "        for vector_type in vector_types:\n",
    "            for model_type in model_types:\n",
    "                if skip > 0:\n",
    "                    skip = skip - 1\n",
    "                    continue\n",
    "                precision, recall, f1_score = run_model(data, \n",
    "                                                        oversampling_rate, \n",
    "                                                        model_type, \n",
    "                                                        vector_type, \n",
    "                                                        embed_size, \n",
    "                                                        True, \n",
    "                                                        \"dumps/%s_%d_%s_%s.csv\" % (data, oversampling_rate, vector_type, model_type))\n",
    "                print \"Results done for %s %d with vector type %s and model %s\" % (data, oversampling_rate, vector_type, model_type)\n",
    "                results_table_6.append({\n",
    "                    \"dataset\": data,\n",
    "                    \"oversampling_rate\": oversampling_rate,\n",
    "                    \"vector_type\": vector_type,\n",
    "                    \"model_type\": model_type,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"f1_score\": f1_score\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e55666fbbf217aa3df372b978577f47b6009e2f78e2ec76a584f49cd54a1e62c"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
