{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/bin/python2.7')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os, sys, getopt, pickle, csv, sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, make_scorer, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble  import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import argparse\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score    \n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [ 'svm', 'naive', 'lr', 'random_forest']\n",
    "NO_OF_FOLDS = 10\n",
    "MODEL_TYPE = \"all\"\n",
    "HASH_REMOVE = None\n",
    "LABEL_ENCODING_TWITTER = {'racism':0,'sexism':1,'none':2}\n",
    "LABEL_ENCODING_FORMSPRING = {'none':0,'bully':1}\n",
    "LABEL_ENCODING_WIKI = {'none':0,'attack':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    x_text = []\n",
    "    labels = []\n",
    "    for i in range(len(data)):\n",
    "        if(HASH_REMOVE):\n",
    "            x_text.append(p.tokenize((data[i]['text']).encode('utf-8')))\n",
    "        else:\n",
    "            x_text.append(data[i]['text'])\n",
    "        labels.append(data[i]['label'])\n",
    "    return x_text,labels\n",
    "\n",
    "def get_filename(dataset):\n",
    "    global N_CLASS, HASH_REMOVE, LABEL_ENCODING\n",
    "    if(dataset==\"twitter\"):\n",
    "        filename = \"data/twitter_data.pkl\"\n",
    "        N_CLASS = 3\n",
    "        LABEL_ENCODING = LABEL_ENCODING_TWITTER\n",
    "        HASH_REMOVE = False\n",
    "    elif(dataset==\"formspring\"):\n",
    "        N_CLASS = 2\n",
    "        LABEL_ENCODING = LABEL_ENCODING_FORMSPRING\n",
    "        filename = \"data/formspring_data.pkl\"\n",
    "        HASH_REMOVE = False\n",
    "    elif(dataset==\"wiki\"):\n",
    "        N_CLASS = 2\n",
    "        LABEL_ENCODING = LABEL_ENCODING_WIKI\n",
    "        filename = \"data/wiki_data.pkl\"\n",
    "        HASH_REMOVE = False\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scores(y_true, y_pred):\n",
    "    #dict1 = {'racism':0,'sexism':1,'none':2}\n",
    "    if(data == \"twitter\"):\n",
    "        scores = np.array([ \n",
    "                    precision_score(y_true, y_pred, average=None, labels=[2, 0, 1]), \n",
    "                    recall_score(y_true, y_pred,  average=None, labels=[2, 0, 1]),\n",
    "                    f1_score(y_true, y_pred, average=None, labels=[2, 0, 1])])\n",
    "    else:\n",
    "        scores = np.array([ \n",
    "                    precision_score(y_true, y_pred, average=None), \n",
    "                    recall_score(y_true, y_pred,  average=None),\n",
    "                    f1_score(y_true, y_pred, average=None)])\n",
    "    return scores\n",
    "    \n",
    "def print_scores(scores):\n",
    "    for i in range(N_CLASS):\n",
    "        scores_class_prec = []\n",
    "        scores_class_rec = []\n",
    "        scores_class_f1 = []\n",
    "        for foldscore in scores:\n",
    "            scores_class_prec.append(foldscore[i])\n",
    "            scores_class_rec.append(foldscore[i])\n",
    "            scores_class_f1.append(foldscore[i])\n",
    "        scores_class_prec = np.array(scores_class_prec)\n",
    "        scores_class_rec = np.array(scores_class_rec)\n",
    "        scores_class_f1 = np.array(scores_class_f1)\n",
    "        class_string = \"\"\n",
    "        for key, value in LABEL_ENCODING.items():\n",
    "            if(value == i):\n",
    "                class_string = key\n",
    "        print \"Precision Class %s (avg): %0.3f (+/- %0.3f)\" % (class_string,scores_class_prec.mean(), scores_class_prec.std() * 2)\n",
    "        print \"Recall Class %s (avg): %0.3f (+/- %0.3f)\" % (class_string,scores_class_rec.mean(), scores_class_rec.std() * 2)\n",
    "        print \"F1_score Class %s (avg): %0.3f (+/- %0.3f)\" % (class_string,scores_class_f1.mean(), scores_class_f1.std() * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recall' 'recall' 'recall']\n"
     ]
    }
   ],
   "source": [
    "arr = []\n",
    "arr.append(np.hstack(np.array([\"precision\",\"recall\",\"F1\"])))\n",
    "arr.append(np.hstack(np.array([\"precision\",\"recall\",\"F1\"])))\n",
    "arr.append(np.hstack(np.array([\"precision\",\"recall\",\"F1\"])))\n",
    "arr = np.array(arr)\n",
    "print \"%s\" % str(arr[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_model(X, Y, model_type):\n",
    "    X, Y = shuffle(X, Y, random_state=42)\n",
    "    print \"Model Type:\", model_type\n",
    "    kf = KFold(n_splits=NO_OF_FOLDS)\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        Y = np.asarray(Y)\n",
    "        model = get_model(model_type)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        curr_scores = get_scores(y_test, y_pred) \n",
    "        #[\n",
    "        #   [precision, precision, precision], -> length == number of classes\n",
    "        #   [recall, recall, recall], \n",
    "        #   [f1, f1, f1]\n",
    "        #]\n",
    "        scores.append(curr_scores)\n",
    "        # [\n",
    "        #   [precision, ... , precision] -> len == number of folds\n",
    "        #   [recall, ... , recall]\n",
    "        #   [f1, ... , f1]\n",
    "        # ]\n",
    "    print_scores(np.array(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(m_type):\n",
    "    if m_type == 'lr':\n",
    "        logreg = LogisticRegression(class_weight=\"balanced\")\n",
    "    elif m_type == 'naive':\n",
    "        logreg =  MultinomialNB()\n",
    "    elif m_type == \"random_forest\":\n",
    "        logreg = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "    elif m_type == \"svm\":\n",
    "        logreg = LinearSVC(class_weight=\"balanced\")\n",
    "    else:\n",
    "        print \"ERROR: Please specify a correst model\"\n",
    "        return None\n",
    "    return logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x_text, labels, MODEL_TYPE):\n",
    "    \n",
    "    if(WORD):\n",
    "        print(\"Using word based features\")\n",
    "        bow_transformer = CountVectorizer(analyzer=\"word\",max_features = 10000,stop_words='english').fit(x_text)\n",
    "        comments_bow = bow_transformer.transform(x_text)\n",
    "        tfidf_transformer = TfidfTransformer(norm = 'l2').fit(comments_bow)\n",
    "        comments_tfidf = tfidf_transformer.transform(comments_bow)\n",
    "        features = comments_tfidf\n",
    "    else: \n",
    "        print(\"Using char n-grams based features\")\n",
    "        bow_transformer = CountVectorizer(max_features = 10000, ngram_range = (1,2)).fit(x_text)\n",
    "        comments_bow = bow_transformer.transform(x_text)\n",
    "        tfidf_transformer = TfidfTransformer(norm = 'l2').fit(comments_bow)\n",
    "        comments_tfidf = tfidf_transformer.transform(comments_bow)\n",
    "        features = comments_tfidf\n",
    "    \n",
    "    if(data == \"twitter\"):\n",
    "        labels = np.array([LABEL_ENCODING_TWITTER[b] for b in labels])\n",
    "    \n",
    "    from collections import Counter\n",
    "    print(Counter(labels))\n",
    "    \n",
    "    if(MODEL_TYPE != \"all\"):\n",
    "        classification_model(features, labels, MODEL_TYPE)\n",
    "    else:\n",
    "        for model_type in models:\n",
    "            classification_model(features, labels, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Using char n-grams based features\n",
      "Counter({0: 11997, 1: 776})\n",
      "Model Type: svm\n",
      "Precision Class 1 (avg): 0.466 (+/- 0.109)\n",
      "Recall Class 1 (avg): 0.503 (+/- 0.122)\n",
      "F1_score Class 1 (avg): 0.483 (+/- 0.104)\n",
      "Model Type: naive\n",
      "Precision Class 1 (avg): 0.850 (+/- 0.640)\n",
      "Recall Class 1 (avg): 0.015 (+/- 0.015)\n",
      "F1_score Class 1 (avg): 0.030 (+/- 0.028)\n",
      "Model Type: lr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.conda/envs/cyberbullying/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/paul/.conda/envs/cyberbullying/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Class 1 (avg): 0.410 (+/- 0.099)\n",
      "Recall Class 1 (avg): 0.626 (+/- 0.131)\n",
      "F1_score Class 1 (avg): 0.495 (+/- 0.104)\n",
      "Model Type: random_forest\n",
      "Precision Class 1 (avg): 0.775 (+/- 0.196)\n",
      "Recall Class 1 (avg): 0.168 (+/- 0.064)\n",
      "F1_score Class 1 (avg): 0.274 (+/- 0.087)\n"
     ]
    }
   ],
   "source": [
    "data = \"formspring\"\n",
    "WORD =  False\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Using word based features\n",
      "Counter({0: 11997, 1: 776})\n",
      "Model Type: svm\n",
      "Precision Class 1 (avg): 0.415 (+/- 0.089)\n",
      "Recall Class 1 (avg): 0.525 (+/- 0.132)\n",
      "F1_score Class 1 (avg): 0.463 (+/- 0.100)\n",
      "Model Type: naive\n",
      "Precision Class 1 (avg): 0.575 (+/- 0.950)\n",
      "Recall Class 1 (avg): 0.013 (+/- 0.029)\n",
      "F1_score Class 1 (avg): 0.025 (+/- 0.055)\n",
      "Model Type: lr\n",
      "Precision Class 1 (avg): 0.407 (+/- 0.079)\n",
      "Recall Class 1 (avg): 0.617 (+/- 0.127)\n",
      "F1_score Class 1 (avg): 0.489 (+/- 0.084)\n",
      "Model Type: random_forest\n",
      "Precision Class 1 (avg): 0.696 (+/- 0.304)\n",
      "Recall Class 1 (avg): 0.162 (+/- 0.075)\n",
      "F1_score Class 1 (avg): 0.261 (+/- 0.112)\n"
     ]
    }
   ],
   "source": [
    "data = \"formspring\"\n",
    "WORD = True\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Using char n-grams based features\n",
      "Counter({2: 11036, 1: 3117, 0: 1937})\n",
      "Model Type: svm\n",
      "Precision Class 1 (avg): 0.731 (+/- 0.057)\n",
      "Recall Class 1 (avg): 0.783 (+/- 0.063)\n",
      "F1_score Class 1 (avg): 0.755 (+/- 0.038)\n",
      "Precision Class 2 (avg): 0.786 (+/- 0.055)\n",
      "Recall Class 2 (avg): 0.736 (+/- 0.067)\n",
      "F1_score Class 2 (avg): 0.759 (+/- 0.040)\n",
      "Model Type: naive\n",
      "Precision Class 1 (avg): 0.790 (+/- 0.063)\n",
      "Recall Class 1 (avg): 0.537 (+/- 0.058)\n",
      "F1_score Class 1 (avg): 0.638 (+/- 0.046)\n",
      "Precision Class 2 (avg): 0.910 (+/- 0.056)\n",
      "Recall Class 2 (avg): 0.455 (+/- 0.089)\n",
      "F1_score Class 2 (avg): 0.605 (+/- 0.082)\n",
      "Model Type: lr\n",
      "Precision Class 1 (avg): 0.737 (+/- 0.058)\n",
      "Recall Class 1 (avg): 0.781 (+/- 0.068)\n",
      "F1_score Class 1 (avg): 0.758 (+/- 0.050)\n",
      "Precision Class 2 (avg): 0.825 (+/- 0.048)\n",
      "Recall Class 2 (avg): 0.641 (+/- 0.084)\n",
      "F1_score Class 2 (avg): 0.721 (+/- 0.059)\n",
      "Model Type: random_forest\n",
      "Precision Class 1 (avg): 0.789 (+/- 0.076)\n",
      "Recall Class 1 (avg): 0.695 (+/- 0.082)\n",
      "F1_score Class 1 (avg): 0.739 (+/- 0.072)\n",
      "Precision Class 2 (avg): 0.897 (+/- 0.041)\n",
      "Recall Class 2 (avg): 0.565 (+/- 0.083)\n",
      "F1_score Class 2 (avg): 0.693 (+/- 0.070)\n"
     ]
    }
   ],
   "source": [
    "data = \"twitter\"\n",
    "WORD = False\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Using word based features\n",
      "Counter({2: 11036, 1: 3117, 0: 1937})\n",
      "Model Type: svm\n",
      "Precision Class 1 (avg): 0.735 (+/- 0.081)\n",
      "Recall Class 1 (avg): 0.786 (+/- 0.067)\n",
      "F1_score Class 1 (avg): 0.758 (+/- 0.043)\n",
      "Precision Class 2 (avg): 0.803 (+/- 0.044)\n",
      "Recall Class 2 (avg): 0.744 (+/- 0.052)\n",
      "F1_score Class 2 (avg): 0.772 (+/- 0.037)\n",
      "Model Type: naive\n",
      "Precision Class 1 (avg): 0.797 (+/- 0.077)\n",
      "Recall Class 1 (avg): 0.529 (+/- 0.068)\n",
      "F1_score Class 1 (avg): 0.635 (+/- 0.062)\n",
      "Precision Class 2 (avg): 0.904 (+/- 0.035)\n",
      "Recall Class 2 (avg): 0.469 (+/- 0.056)\n",
      "F1_score Class 2 (avg): 0.617 (+/- 0.051)\n",
      "Model Type: lr\n",
      "Precision Class 1 (avg): 0.741 (+/- 0.065)\n",
      "Recall Class 1 (avg): 0.786 (+/- 0.056)\n",
      "F1_score Class 1 (avg): 0.762 (+/- 0.042)\n",
      "Precision Class 2 (avg): 0.832 (+/- 0.039)\n",
      "Recall Class 2 (avg): 0.663 (+/- 0.083)\n",
      "F1_score Class 2 (avg): 0.738 (+/- 0.062)\n",
      "Model Type: random_forest\n",
      "Precision Class 1 (avg): 0.774 (+/- 0.057)\n",
      "Recall Class 1 (avg): 0.746 (+/- 0.084)\n",
      "F1_score Class 1 (avg): 0.759 (+/- 0.048)\n",
      "Precision Class 2 (avg): 0.873 (+/- 0.044)\n",
      "Recall Class 2 (avg): 0.647 (+/- 0.077)\n",
      "F1_score Class 2 (avg): 0.742 (+/- 0.055)\n"
     ]
    }
   ],
   "source": [
    "data = \"twitter\"\n",
    "WORD = True\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Using char n-grams based features\n",
      "Counter({0: 102274, 1: 13590})\n",
      "Model Type: svm\n",
      "Precision Class 1 (avg): 0.591 (+/- 0.025)\n",
      "Recall Class 1 (avg): 0.823 (+/- 0.019)\n",
      "F1_score Class 1 (avg): 0.688 (+/- 0.018)\n",
      "Model Type: naive\n",
      "Precision Class 1 (avg): 0.839 (+/- 0.010)\n",
      "Recall Class 1 (avg): 0.554 (+/- 0.028)\n",
      "F1_score Class 1 (avg): 0.667 (+/- 0.021)\n",
      "Model Type: lr\n",
      "Precision Class 1 (avg): 0.602 (+/- 0.024)\n",
      "Recall Class 1 (avg): 0.845 (+/- 0.022)\n",
      "F1_score Class 1 (avg): 0.703 (+/- 0.017)\n",
      "Model Type: random_forest\n",
      "Precision Class 1 (avg): 0.888 (+/- 0.017)\n",
      "Recall Class 1 (avg): 0.549 (+/- 0.028)\n",
      "F1_score Class 1 (avg): 0.678 (+/- 0.024)\n"
     ]
    }
   ],
   "source": [
    "data = \"wiki\"\n",
    "WORD = False\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Using word based features\n",
      "Counter({0: 102274, 1: 13590})\n",
      "Model Type: svm\n",
      "Precision Class 1 (avg): 0.591 (+/- 0.025)\n",
      "Recall Class 1 (avg): 0.819 (+/- 0.028)\n",
      "F1_score Class 1 (avg): 0.686 (+/- 0.020)\n",
      "Model Type: naive\n",
      "Precision Class 1 (avg): 0.898 (+/- 0.016)\n",
      "Recall Class 1 (avg): 0.521 (+/- 0.036)\n",
      "F1_score Class 1 (avg): 0.659 (+/- 0.028)\n",
      "Model Type: lr\n",
      "Precision Class 1 (avg): 0.620 (+/- 0.027)\n",
      "Recall Class 1 (avg): 0.834 (+/- 0.024)\n",
      "F1_score Class 1 (avg): 0.711 (+/- 0.021)\n",
      "Model Type: random_forest\n",
      "Precision Class 1 (avg): 0.811 (+/- 0.022)\n",
      "Recall Class 1 (avg): 0.661 (+/- 0.029)\n",
      "F1_score Class 1 (avg): 0.729 (+/- 0.023)\n"
     ]
    }
   ],
   "source": [
    "data = \"wiki\"\n",
    "WORD = True\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d00281ea772942db40b05b5b7f3394a1f0c0f473f510a1c8681664b48ba3c89d"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
